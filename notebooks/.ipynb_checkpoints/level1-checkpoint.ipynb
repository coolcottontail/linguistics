{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the closest document using Term Freq-Inverse Document Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk\n",
    "import numpy as np\n",
    "from math import log10, sqrt\n",
    "from string import punctuation\n",
    "from sklearn.metrics import jaccard_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/1.txt :\n",
      "hello guys, please do your task seriously\n",
      "\n",
      "File data/2.txt :\n",
      "number one is so difficult. I couldn' t hold it anymore\n",
      "\n",
      "File data/3.txt :\n",
      "math assignment really makes me sleepy. please wake me up now\n",
      "\n",
      "\n",
      "Search file data/input.txt :\n",
      "history assignment makes me so sleepy. please wake me up later \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#list of existing documents\n",
    "list_of_original_files = [\"data/1.txt\",\"data/2.txt\", \"data/3.txt\"]\n",
    "#the input document to search \n",
    "inputFile = \"data/input.txt\"\n",
    "\n",
    "for file in list_of_original_files:\n",
    "    f = open(file)\n",
    "    print(\"File\", file, \":\")\n",
    "    print(f.read())\n",
    "    f.close()\n",
    "\n",
    "f = open(inputFile)\n",
    "print(\"\\nSearch file\", inputFile, \":\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of unique words (terms) from exisiting documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'seriously', 'task', 'do', 'please', 'hello', 'guys', 'is', 'one', 'i', 'couldn', 'so', 'anymore', 'hold', 'number', 't', 'it', 'difficult', 'really', 'up', 'me', 'wake', 'now', 'assignment', 'sleepy', 'please', 'math', 'makes']\n"
     ]
    }
   ],
   "source": [
    "def get_unique_words_from_doc(file):\n",
    "    # Extract unique words (unigram, bigram, trigram) from the MASTER DOCUMENT\n",
    "    with open(file, 'r') as f:\n",
    "        all_text = f.read().replace('\\n', ' ')\n",
    "    # Replace single quote (\" ' \") into single white space\n",
    "    allText = all_text.replace(\"'\", \" \")\n",
    "\n",
    "    # Get a set of unique words, removing all punctuation\n",
    "    return set(allText.translate(str.maketrans('', '', punctuation)).lower().split())\n",
    "\n",
    "uniqueWords = []\n",
    "for file in list_of_original_files:\n",
    "        uniqueWords += get_unique_words_from_doc(file)\n",
    "        \n",
    "print(uniqueWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all stop words from the unique words set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seriously', 'task', 'please', 'hello', 'guys', 'one', 'anymore', 'hold', 'number', 'difficult', 'really', 'wake', 'assignment', 'sleepy', 'please', 'math', 'makes']\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def get_unique_word_without_stopwords(uniqueWords):    \n",
    "    noStopWords = []\n",
    "    for uniqueWord in uniqueWords:\n",
    "        if (not uniqueWord in stopwords):\n",
    "            noStopWords.append(uniqueWord)\n",
    "    return noStopWords\n",
    "uniqueWordList = get_unique_word_without_stopwords(uniqueWords)\n",
    "\n",
    "print(uniqueWordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute IDF of unique words in the original files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seriously': 1.4771212547196624, 'task': 1.4771212547196624, 'please': 1.4066802324977494, 'hello': 1.4771212547196624, 'guys': 1.4771212547196624, 'one': 1.4771212547196624, 'anymore': 1.4771212547196624, 'hold': 1.4771212547196624, 'number': 1.4771212547196624, 'difficult': 1.4771212547196624, 'really': 1.4771212547196624, 'wake': 1.4771212547196624, 'assignment': 1.4771212547196624, 'sleepy': 1.4771212547196624, 'math': 1.4771212547196624, 'makes': 1.4771212547196624}\n"
     ]
    }
   ],
   "source": [
    "def get_df_of_words(uniqueWordList, list_of_original_files):\n",
    "    listofdocs = []                       \n",
    "    for file in list_of_original_files:\n",
    "        listofdocs.append(get_unique_words_from_doc(file))\n",
    "    dfDict = {}\n",
    "    for uniqueWord in uniqueWordList:\n",
    "        counter = 0\n",
    "        for doc in listofdocs:\n",
    "            if (uniqueWord in doc):\n",
    "                counter += 1\n",
    "        dfDict[uniqueWord] = counter\n",
    "    return dfDict\n",
    "\n",
    "def get_idf_of_words(uniqueWordList, list_of_original_files):\n",
    "    docsize = len(list_of_original_files)\n",
    "    idfDict = get_df_of_words(uniqueWordList, list_of_original_files)\n",
    "    for word in uniqueWordList:\n",
    "        if idfDict[word] == 0:\n",
    "            idfDict[word] = 1\n",
    "        else:\n",
    "            idfDict[word] = 1 + (log10(docsize / idfDict[word]))\n",
    "    return idfDict\n",
    "\n",
    "idfUniqueWordList = get_idf_of_words(uniqueWordList, list_of_original_files)\n",
    "\n",
    "print(idfUniqueWordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute TF of unique words in the original files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data/1.txt': {'seriously': 0.2110173221028089, 'task': 0.2110173221028089, 'please': 0.20095431892824991, 'hello': 0.2110173221028089, 'guys': 0.2110173221028089, 'one': 0.0, 'anymore': 0.0, 'hold': 0.0, 'number': 0.0, 'difficult': 0.0, 'really': 0.0, 'wake': 0.0, 'assignment': 0.0, 'sleepy': 0.0, 'math': 0.0, 'makes': 0.0}, 'data/2.txt': {'seriously': 0.0, 'task': 0.0, 'please': 0.0, 'hello': 0.0, 'guys': 0.0, 'one': 0.1342837504290602, 'anymore': 0.1342837504290602, 'hold': 0.1342837504290602, 'number': 0.1342837504290602, 'difficult': 0.1342837504290602, 'really': 0.0, 'wake': 0.0, 'assignment': 0.0, 'sleepy': 0.0, 'math': 0.0, 'makes': 0.0}, 'data/3.txt': {'seriously': 0.0, 'task': 0.0, 'please': 0.14066802324977495, 'hello': 0.0, 'guys': 0.0, 'one': 0.0, 'anymore': 0.0, 'hold': 0.0, 'number': 0.0, 'difficult': 0.0, 'really': 0.14771212547196624, 'wake': 0.14771212547196624, 'assignment': 0.14771212547196624, 'sleepy': 0.14771212547196624, 'math': 0.14771212547196624, 'makes': 0.14771212547196624}}\n",
      "{'seriously': 0.0, 'task': 0.0, 'please': 0.14066802324977495, 'hello': 0.0, 'guys': 0.0, 'one': 0.0, 'anymore': 0.0, 'hold': 0.0, 'number': 0.0, 'difficult': 0.0, 'really': 0.0, 'wake': 0.14771212547196624, 'assignment': 0.14771212547196624, 'sleepy': 0.14771212547196624, 'math': 0.0, 'makes': 0.14771212547196624}\n"
     ]
    }
   ],
   "source": [
    "def get_tf_of_word(listofwords, word):\n",
    "    return listofwords.count(word)/len(listofwords)\n",
    "\n",
    "def get_tf_idf_for_file(file, uniqueWordList):\n",
    "    listofwords = list(get_unique_words_from_doc(file))\n",
    "    idfDict = get_idf_of_words(uniqueWordList, list_of_original_files)\n",
    "    tfIdfDict = {}\n",
    "    for word in uniqueWordList:\n",
    "        tfIdfDict[word] = get_tf_of_word(listofwords, word) * idfDict[word]\n",
    "    return tfIdfDict\n",
    "\n",
    "tfIdfDict = {}\n",
    "for file in list_of_original_files:\n",
    "        tfIdfDict[file] = get_tf_idf_for_file(file, uniqueWordList)\n",
    "        \n",
    "inputFileTfIdf = get_tf_idf_for_file(inputFile, uniqueWordList)\n",
    "print(tfIdfDict)\n",
    "print(inputFileTfIdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get closest document using euclid distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data/1.txt', 0.5221662607338883), ('data/2.txt', 0.46584450380170056), ('data/3.txt', 0.20889649116941097)]\n",
      "\n",
      "Here's the result:\n",
      "Document  data/3.txt  is closest to  data/input.txt\n"
     ]
    }
   ],
   "source": [
    "def euclid_distance(idf1, idf2, uniqueWordList):\n",
    "    sum = 0\n",
    "    for word in uniqueWordList:\n",
    "        sum += (idf1[word] - idf2[word])*(idf1[word] - idf2[word])\n",
    "    return sqrt(sum)\n",
    "\n",
    "distanceList = [(f,euclid_distance(inputFileTfIdf, tfIdfDict[f], uniqueWordList)) for f in list_of_original_files]\n",
    "print(distanceList)\n",
    "\n",
    "#get the minimal from the tuple list, using second element of tuple as key\n",
    "matchTuple = min(distanceList, key = lambda x: x[1])\n",
    "print(\"\\nHere's the result:\\nDocument \", matchTuple[0], \" is closest to \", inputFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
